# -*- coding: utf-8 -*-
"""TABLE_III_MNISTVGG.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qmnUS6zB_elj-QQ7loDu2rND9bukJEoK

# CW3: VGG Model
"""

import tensorflow as tf

from tensorflow.keras import datasets, layers, models, optimizers, callbacks
from keras.callbacks import EarlyStopping, ModelCheckpoint

import matplotlib.pyplot as plt

# !pip install --upgrade tensorflow

"""# Building ConvNet Configuration Types

https://arxiv.org/pdf/1409.1556.pdf (Page 3, Tables 1 & 2)
"""

mnist_shape = (28,28,1)
mnist_classes = 10

"""## Configuration: A (11 weight layers)"""

model_A = models.Sequential()

model_A.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=mnist_shape))
model_A.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))

model_A.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
model_A.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))

model_A.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))
model_A.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))
model_A.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))

model_A.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_A.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_A.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))

model_A.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_A.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_A.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))
model_A.add(layers.Dropout(0.5))

model_A.add(layers.Flatten())
model_A.add(layers.Dense(4096, activation='relu'))
model_A.add(layers.Dense(4096, activation='relu'))
model_A.add(layers.Dense(mnist_classes, activation='softmax'))

model_A.summary()
# Total params: 132,863,336 for ILSVRC
# Total params: 28,144,010 for CIFAR 
# Total params: 28,142,858 for MNIST

"""## Configuration: B (13 weight layers)"""

model_B = models.Sequential()

model_B.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=mnist_shape))
model_B.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))
model_B.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))

model_B.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
model_B.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
model_B.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))

model_B.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))
model_B.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))
model_B.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))

model_B.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_B.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_B.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))

model_B.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_B.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_B.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))
model_B.add(layers.Dropout(0.5))

model_B.add(layers.Flatten())
model_B.add(layers.Dense(4096, activation='relu'))
model_B.add(layers.Dense(4096, activation='relu'))
model_B.add(layers.Dense(mnist_classes, activation='softmax'))

model_B.summary()
# Total params: 133,047,848 for ILSVRC
# Total params: 28,328,522 for CIFAR
# Total params: 28,327,370 for MNIST

"""## Configuration: C (16 weight layers)"""

model_C = models.Sequential()

model_C.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=mnist_shape))
model_C.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))
model_C.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))

model_C.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
model_C.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
model_C.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))

model_C.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))
model_C.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))
model_C.add(layers.Conv2D(256, (1, 1), padding='same', activation='relu'))
model_C.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))

model_C.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_C.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_C.add(layers.Conv2D(512, (1, 1), padding='same', activation='relu'))
model_C.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))

model_C.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_C.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_C.add(layers.Conv2D(512, (1, 1), padding='same', activation='relu'))
model_C.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))
model_C.add(layers.Dropout(0.5))

model_C.add(layers.Flatten())
model_C.add(layers.Dense(4096, activation='relu'))
model_C.add(layers.Dense(4096, activation='relu'))
model_C.add(layers.Dense(mnist_classes, activation='softmax'))

model_C.summary()
# Total params: 133,638,952 for ILSVRC
# Total params: 28,919,626 for CIFAR
# Total params: 28,918,474 for MNIST

"""## Configuration: D (16 weight layers)"""

model_D = models.Sequential()

model_D.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=mnist_shape))
model_D.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))
model_D.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))

model_D.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
model_D.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
model_D.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))

model_D.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))
model_D.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))
model_D.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))
model_D.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))

model_D.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_D.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_D.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_D.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))

model_D.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_D.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_D.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_D.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))
model_D.add(layers.Dropout(0.5))

model_D.add(layers.Flatten())
model_D.add(layers.Dense(4096, activation='relu'))
model_D.add(layers.Dense(4096, activation='relu'))
model_D.add(layers.Dense(mnist_classes, activation='softmax'))

model_D.summary()
# Total params: 138,357,544 for ILSVRC
# Total params: 33,638,218 for CIFAR
# Total params: 33,637,066 for MNIST

"""## Configuration: E (19 weight layers, approx 144 million parameters)"""

model_E = models.Sequential()

model_E.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=mnist_shape))
model_E.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))
model_E.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))

model_E.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
model_E.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
model_E.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))

model_E.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))
model_E.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))
model_E.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))
model_E.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))
model_E.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))

model_E.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_E.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_E.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_E.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_E.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))

model_E.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_E.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_E.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_E.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))
model_E.add(layers.MaxPool2D((2, 2), strides=2, padding='same'))
model_E.add(layers.Dropout(0.5))

model_E.add(layers.Flatten())
model_E.add(layers.Dense(4096, activation='relu'))
model_E.add(layers.Dense(4096, activation='relu'))
model_E.add(layers.Dense(mnist_classes, activation='softmax'))

model_E.summary()
# Total params: 143,667,240 for ILSVRC
# Total params: 38,947,914 for CIFAR
# Total params: 38,946,762 for MNIST

"""# Implementation of Models on Datasets"""

import tensorflow as tf
from tensorflow.keras import datasets
import matplotlib.pyplot as plt

"""## MNIST"""

(train_images_mnist, train_labels_mnist), (test_images_mnist, test_labels_mnist) = datasets.mnist.load_data(path='mnist.npz')

# Normalize pixel values to be between 0 and 1
train_images_mnist, test_images_mnist = train_images_mnist / 255.0, test_images_mnist / 255.0

class_names = ['0', '1', '2', '3', '4',
               '5', '6', '7', '8', '9']

plt.figure(figsize=(20,20))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images_mnist[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels_mnist[i]])
plt.show()

# reshape dataset to have a single channel
train_images_mnist = train_images_mnist.reshape((train_images_mnist.shape[0], 28, 28, 1))
test_images_mnist = test_images_mnist.reshape((test_images_mnist.shape[0], 28, 28, 1))

# print(train_images_mnist.shape)
# print(test_images_mnist)
print('test_images_cifar:', test_images_mnist.shape)
print('test_labels_cifar:', test_labels_mnist.shape)

sgd = optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)

model_A.compile(optimizer=sgd,
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])


history = model_A.fit(train_images_mnist, 
                      train_labels_mnist, 
                      batch_size=256,
                      epochs=200,
                      validation_data=(test_images_mnist, test_labels_mnist))

plt.plot(history.history["accuracy"])
plt.plot(history.history['val_accuracy'])
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title("model accuracy")
plt.ylabel("Accuracy")
plt.xlabel("Epoch")
plt.legend(["Accuracy","Validation Accuracy","loss","Validation Loss"])
plt.show()

sgd = optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)

model_B.compile(optimizer=sgd,
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])


history = model_B.fit(train_images_mnist, 
                      train_labels_mnist, 
                      batch_size=256,
                      epochs=200,
                      validation_data=(test_images_mnist, test_labels_mnist))

plt.plot(history.history["accuracy"])
plt.plot(history.history['val_accuracy'])
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title("model accuracy")
plt.ylabel("Accuracy")
plt.xlabel("Epoch")
plt.legend(["Accuracy","Validation Accuracy","loss","Validation Loss"])
plt.show()

sgd = optimizers.SGD(learning_rate=0.005, decay=1e-6, momentum=0.9, nesterov=True)

model_C.compile(optimizer=sgd,
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])


history = model_C.fit(train_images_mnist, 
                      train_labels_mnist, 
                      batch_size=256,
                      epochs=200,
                      validation_data=(test_images_mnist, test_labels_mnist))

plt.plot(history.history["accuracy"])
plt.plot(history.history['val_accuracy'])
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title("model accuracy")
plt.ylabel("Accuracy")
plt.xlabel("Epoch")
plt.legend(["Accuracy","Validation Accuracy","loss","Validation Loss"])
plt.show()

sgd = optimizers.SGD(learning_rate=0.005, decay=1e-6, momentum=0.9, nesterov=True)

model_D.compile(optimizer=sgd,
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])


history = model_D.fit(train_images_mnist, 
                      train_labels_mnist, 
                      batch_size=256,
                      epochs=200,
                      validation_data=(test_images_mnist, test_labels_mnist))

plt.plot(history.history["accuracy"])
plt.plot(history.history['val_accuracy'])
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title("model accuracy")
plt.ylabel("Accuracy")
plt.xlabel("Epoch")
plt.legend(["Accuracy","Validation Accuracy","loss","Validation Loss"])
plt.show()

sgd = optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)

model_E.compile(optimizer=sgd,
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])


history = model_E.fit(train_images_mnist, 
                      train_labels_mnist, 
                      batch_size=256,
                      epochs=200,
                      validation_data=(test_images_mnist, test_labels_mnist))

plt.plot(history.history["accuracy"])
plt.plot(history.history['val_accuracy'])
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title("model accuracy")
plt.ylabel("Accuracy")
plt.xlabel("Epoch")
plt.legend(["Accuracy","Validation Accuracy","loss","Validation Loss"])
plt.show()